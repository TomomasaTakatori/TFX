{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimatorで利用する\n",
    "> tftで作成したデータで訓練して、予測時にその処理を当てはめられる様にEstimatorを書く。\n",
    "\n",
    "## 手順\n",
    "    1. TFRecordからDataset作成する関数を書く\n",
    "    2. metadataからFeature columnsを作成する関数を書く\n",
    "    3. Estimatorを訓練する\n",
    "    4. transform fnを適用したServing input fnを作成し、estimatorをsaved modelとして保存\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import os\n",
    "\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "\n",
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定数を定義\n",
    "TRANSFORM_ARTEFACTS_DIR = 'transform_fn'\n",
    "TARGET_FEATURE_NAME = 'fare_amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_metadata = metadata_io.read_metadata(\n",
    "        os.path.join(TRANSFORM_ARTEFACTS_DIR,\"transformed_metadata\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = transformed_metadata.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema(feature {\n",
       "  name: \"dropofflat\"\n",
       "  type: INT\n",
       "  int_domain {\n",
       "    min: 0\n",
       "    max: 5\n",
       "    is_categorical: true\n",
       "  }\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"dropofflon\"\n",
       "  type: INT\n",
       "  int_domain {\n",
       "    min: 0\n",
       "    max: 5\n",
       "    is_categorical: true\n",
       "  }\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"fare_amount\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"key\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"passengers\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"pickuplat\"\n",
       "  type: INT\n",
       "  int_domain {\n",
       "    min: 0\n",
       "    max: 5\n",
       "    is_categorical: true\n",
       "  }\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"pickuplon\"\n",
       "  type: INT\n",
       "  int_domain {\n",
       "    min: 0\n",
       "    max: 4\n",
       "    is_categorical: true\n",
       "  }\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_schema = a.as_feature_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropofflat': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'dropofflon': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'fare_amount': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
       " 'key': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
       " 'passengers': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
       " 'pickuplat': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'pickuplon': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_schema['dropofflat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.int64"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_schema['dropofflat'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropofflat\n",
      "dropofflon\n",
      "fare_amount\n",
      "key\n",
      "passengers\n",
      "pickuplat\n",
      "pickuplon\n"
     ]
    }
   ],
   "source": [
    "for i in column_schema:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_info = a.domains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min: 0\n",
       "max: 5\n",
       "is_categorical: true"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_info['dropofflat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecordからDatasetを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecords_input_fn(files_name_pattern, transformed_metadata,\n",
    "                       mode=tf.estimator.ModeKeys.EVAL,  \n",
    "                       num_epochs=1, \n",
    "                       batch_size=500):\n",
    "    def _divide_func(features):\n",
    "        target = features.pop(TARGET_FEATURE_NAME)\n",
    "        return features, target\n",
    "    dataset = tf.contrib.data.make_batched_features_dataset(\n",
    "        file_pattern=files_name_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transformed_metadata.schema.as_feature_spec(),\n",
    "        reader=tf.data.TFRecordDataset,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=True if mode == tf.estimator.ModeKeys.TRAIN else False,\n",
    "        shuffle_buffer_size=1+(batch_size*2),\n",
    "        prefetch_buffer_size=1\n",
    "    )\n",
    "    dataset = dataset.map(_divide_func)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature columnを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_columns(transformed_metadata):\n",
    "    feature_columns = []\n",
    "    \n",
    "    column_schemas = transformed_metadata.schema.as_feature_spec()\n",
    "    \n",
    "    for feature_name in column_schemas:\n",
    "        if feature_name==TARGET_FEATURE_NAME:\n",
    "            continue\n",
    "        column_schema = column_schemas[feature_name]\n",
    "        \n",
    "        if column_schema.dtype == tf.float32:\n",
    "            feature_columns.append(tf.feature_column.numeric_column(feature_name))\n",
    "        elif column_schema.dtype == tf.int64:\n",
    "\n",
    "            feature_columns.append(\n",
    "                tf.feature_column.categorical_column_with_identity(\n",
    "                feature_name, num_buckets=5+1)\n",
    "                                  )\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参考にしたコード\n",
    "def create_feature_columns(transformed_metadata):\n",
    "    feature_columns = []\n",
    "    \n",
    "    column_schemas = transformed_metadata.schema.column_schemas\n",
    "    \n",
    "    for feature_name in column_schemas:\n",
    "        if feature_name==TARGET_FEATURE_NAME:\n",
    "            cotinue\n",
    "        column_schema = column_schemas[feature_name]\n",
    "        \n",
    "        if isinstance(column_schema._domain, dataset_schema.FloatDomain):\n",
    "            feature_columns.append(tf.feature_column.numeric_column(feature_name))\n",
    "        elif isinstance(column_schema._domain, dataset_schema.IntDomain):\n",
    "            if column_schema._domain._is_categorical==True:\n",
    "                feature_columns.append(\n",
    "                    tf.feature_column.categorical_column_with_identity(\n",
    "                    feature_name, num_buckets=column_schema._domain._max_value+1)\n",
    "                                      )\n",
    "            else:\n",
    "                feature_columns.append(\n",
    "                    tf.feature_column.numeric_columne(feature_name)\n",
    "                )\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator(transformed_metadata, run_config):\n",
    "    feature_cols = create_feature_columns(transformed_metadata)\n",
    "    estimator = tf.estimator.LinearRegressor(feature_cols, config=run_config)\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'tft_output_dir'\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=1982983,\n",
    "    model_dir=model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_files = './tfrecord/train_transformed-00000-of-00001'\n",
    "eval_data_files = './tfrecord/test_transformed-00000-of-00001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainSpec\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=lambda: tfrecords_input_fn(\n",
    "        train_data_files, transformed_metadata,\n",
    "        mode=tf.estimator.ModeKeys.TRAIN,\n",
    "        num_epochs=10,\n",
    "        batch_size=256,\n",
    "    ),\n",
    "    max_steps=2000\n",
    ")\n",
    "\n",
    "# EvalSpec\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=lambda: tfrecords_input_fn(\n",
    "        eval_data_files, transformed_metadata\n",
    "    ),\n",
    "    steps=None,\n",
    "    throttle_secs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 21:12:07.193453 4734801344 estimator.py:209] Using config: {'_model_dir': 'tft_output_dir', '_tf_random_seed': 1982983, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x13be83eb8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "I0806 21:12:07.197669 4734801344 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "I0806 21:12:07.198747 4734801344 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "I0806 21:12:07.200374 4734801344 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "I0806 21:12:07.370188 4734801344 estimator.py:1145] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment started at 12:12:07\n",
      ".......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 21:12:09.330124 4734801344 estimator.py:1147] Done calling model_fn.\n",
      "I0806 21:12:09.332545 4734801344 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I0806 21:12:09.621339 4734801344 monitored_session.py:240] Graph was finalized.\n",
      "I0806 21:12:09.764506 4734801344 session_manager.py:500] Running local_init_op.\n",
      "I0806 21:12:09.817604 4734801344 session_manager.py:502] Done running local_init_op.\n",
      "I0806 21:12:11.483687 4734801344 basic_session_run_hooks.py:606] Saving checkpoints for 0 into tft_output_dir/model.ckpt.\n",
      "I0806 21:12:12.358397 4734801344 basic_session_run_hooks.py:262] loss = 45861.266, step = 1\n",
      "I0806 21:12:15.253170 4734801344 basic_session_run_hooks.py:692] global_step/sec: 34.494\n",
      "I0806 21:12:15.254960 4734801344 basic_session_run_hooks.py:260] loss = 25182.527, step = 101 (2.897 sec)\n",
      "I0806 21:12:15.867171 4734801344 basic_session_run_hooks.py:692] global_step/sec: 162.873\n",
      "I0806 21:12:15.869142 4734801344 basic_session_run_hooks.py:260] loss = 22965.629, step = 201 (0.614 sec)\n",
      "I0806 21:12:16.688933 4734801344 basic_session_run_hooks.py:692] global_step/sec: 121.688\n",
      "I0806 21:12:16.691173 4734801344 basic_session_run_hooks.py:260] loss = 18936.113, step = 301 (0.822 sec)\n",
      "I0806 21:12:16.722327 4734801344 basic_session_run_hooks.py:606] Saving checkpoints for 304 into tft_output_dir/model.ckpt.\n",
      "I0806 21:12:18.062752 4734801344 estimator.py:1145] Calling model_fn.\n",
      "I0806 21:12:25.203041 4734801344 estimator.py:1147] Done calling model_fn.\n",
      "I0806 21:12:25.475547 4734801344 evaluation.py:255] Starting evaluation at 2019-08-06T21:12:25Z\n",
      "I0806 21:12:26.747072 4734801344 monitored_session.py:240] Graph was finalized.\n",
      "I0806 21:12:26.800628 4734801344 saver.py:1280] Restoring parameters from tft_output_dir/model.ckpt-304\n",
      "I0806 21:12:27.191495 4734801344 session_manager.py:500] Running local_init_op.\n",
      "I0806 21:12:27.239941 4734801344 session_manager.py:502] Done running local_init_op.\n",
      "I0806 21:12:29.249921 4734801344 evaluation.py:275] Finished evaluation at 2019-08-06-21:12:29\n",
      "I0806 21:12:29.258723 4734801344 estimator.py:2039] Saving dict for global step 304: average_loss = 179.55942, global_step = 304, label/mean = 11.4478445, loss = 74786.5, prediction/mean = 1.9336512\n",
      "I0806 21:12:29.269650 4734801344 estimator.py:2099] Saving 'checkpoint_path' summary for global step 304: tft_output_dir/model.ckpt-304\n",
      "I0806 21:12:29.525113 4734801344 estimator.py:368] Loss for final step: 23634.576.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................................\n",
      "Experiment finished at 12:12:29\n",
      "\n",
      "Experiment elapsed time: 22.331452 seconds\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "if tf.io.gfile.exists(model_dir):\n",
    "    tf.io.gfile.rmtree(model_dir)\n",
    "\n",
    "estimator = create_estimator(transformed_metadata, run_config)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "time_start = datetime.utcnow() \n",
    "print(\"\")\n",
    "print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "print(\".......................................\") \n",
    "\n",
    "\n",
    "tf.estimator.train_and_evaluate(\n",
    "  estimator,\n",
    "  train_spec,\n",
    "  eval_spec\n",
    ")\n",
    "\n",
    "\n",
    "time_end = datetime.utcnow() \n",
    "print(\".......................................\")\n",
    "print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "print(\"\")\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Export\n",
    "> tfma用にeval saved modelも作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data metadata\n",
    "NUMERIC_FEATURE_NAMES = ['pickuplon','pickuplat', 'dropofflon','dropofflat','passengers']\n",
    "TARGET_FEATURE_NAME = 'fare_amount'\n",
    "KEY_COLUMN = 'key'\n",
    "def create_raw_metadata():  \n",
    "    \n",
    "    raw_data_schema = {}\n",
    "    \n",
    "    # key feature schema\n",
    "    raw_data_schema[KEY_COLUMN]= dataset_schema.ColumnSchema(\n",
    "        tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "    \n",
    "    # target feature schema\n",
    "    raw_data_schema[TARGET_FEATURE_NAME]= dataset_schema.ColumnSchema(\n",
    "        tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "        \n",
    "    # numerical features schema\n",
    "    raw_data_schema.update({ column_name : dataset_schema.ColumnSchema(\n",
    "        tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "                            for column_name in NUMERIC_FEATURE_NAMES})\n",
    "    \n",
    "      # create dataset_metadata given raw_schema\n",
    "    raw_metadata = dataset_metadata.DatasetMetadata(\n",
    "        dataset_schema.Schema(raw_data_schema))\n",
    "    \n",
    "    return raw_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_metaedata = create_raw_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=raw_metaedata.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    from tensorflow_transform.saved import saved_transform_io\n",
    "    \n",
    "    raw_input_features = {\n",
    "        name: tf.compat.v1.placeholder(tf.float32, [None]) for name in NUMERIC_FEATURE_NAMES + [KEY_COLUMN]\n",
    "    }\n",
    "    \n",
    "    _, transformed_features = (\n",
    "        saved_transform_io.partially_apply_saved_transform(\n",
    "            os.path.join(TRANSFORM_ARTEFACTS_DIR, transform_fn_io.TRANSFORM_FN_DIR),\n",
    "            raw_input_features\n",
    "        )\n",
    "    )\n",
    "    return tf.estimator.export.ServingInputReceiver(transformed_features, raw_input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0806 22:07:54.094624 4734801344 saved_transform_io.py:323] partially_apply_saved_transform is deprecated.  Use the transform_raw_features method of the TFTrandformOutput class instead.\n",
      "I0806 22:07:54.173266 4734801344 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
      "I0806 22:07:54.181468 4734801344 estimator.py:1145] Calling model_fn.\n",
      "I0806 22:07:55.013038 4734801344 estimator.py:1147] Done calling model_fn.\n",
      "I0806 22:07:55.015441 4734801344 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
      "I0806 22:07:55.019019 4734801344 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
      "I0806 22:07:55.022450 4734801344 export_utils.py:170] Signatures INCLUDED in export for Predict: ['predict']\n",
      "I0806 22:07:55.023123 4734801344 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
      "I0806 22:07:55.025068 4734801344 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
      "I0806 22:07:55.026631 4734801344 export_utils.py:173] Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "I0806 22:07:55.028818 4734801344 export_utils.py:176] 'serving_default' : Regression input must be a single string Tensor; got {'pickuplon': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'pickuplat': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'dropofflon': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'dropofflat': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'passengers': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'key': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>}\n",
      "I0806 22:07:55.035542 4734801344 export_utils.py:176] 'regression' : Regression input must be a single string Tensor; got {'pickuplon': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'pickuplat': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'dropofflon': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'dropofflat': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'passengers': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'key': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>}\n",
      "W0806 22:07:55.036741 4734801344 export_utils.py:182] Export includes no default signature!\n",
      "I0806 22:07:55.119770 4734801344 saver.py:1280] Restoring parameters from tft_output_dir/model.ckpt-304\n",
      "I0806 22:07:55.246114 4734801344 builder_impl.py:661] Assets added to graph.\n",
      "I0806 22:07:55.247047 4734801344 builder_impl.py:456] No assets to write.\n",
      "I0806 22:07:55.553058 4734801344 builder_impl.py:421] SavedModel written to: tft_output_dir/export/temp-b'1565096874'/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'tft_output_dir/export/1565096874'"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_dir = os.path.join(model_dir, 'export')\n",
    "\n",
    "if tf.io.gfile.exists(export_dir):\n",
    "    tf.io.gfile.rmtree(export_dir)\n",
    "\n",
    "estimator.export_savedmodel(\n",
    "    export_dir_base=export_dir,\n",
    "    serving_input_receiver_fn=serving_input_receiver_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 22:09:06.668709 4734801344 saver.py:1280] Restoring parameters from tft_output_dir/export/1565096874/variables/variables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4761792\n"
     ]
    }
   ],
   "source": [
    "saved_model_dir = 'tft_output_dir/export/1565096874'\n",
    "\n",
    "\n",
    "def estimate_local(instance):\n",
    " \n",
    "    predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "        export_dir=saved_model_dir,\n",
    "        signature_def_key=\"predict\"\n",
    "    )\n",
    "    \n",
    "    instance = dict((k, [v]) for k, v in instance.items())\n",
    "    value = predictor_fn(instance)['predictions'][0][0]\n",
    "    return value\n",
    "\n",
    "instance = {\n",
    "    'dropofflat': -73.987625,\n",
    "    'dropofflon': 40.750617,\n",
    "    'passengers': 0.0,\n",
    "    'pickuplat': -73.971163,\n",
    "    'pickuplon': -73.971163,\n",
    "    'key': 5.0 \n",
    "}\n",
    "\n",
    "prediction = estimate_local(instance)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also export the EvalSavedModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_eval_columns(transformed_metadata):\n",
    "    feature_columns = []\n",
    "    \n",
    "    column_schemas = transformed_metadata.schema.as_feature_spec()\n",
    "    \n",
    "    for feature_name in column_schemas:\n",
    "        column_schema = column_schemas[feature_name]\n",
    "        \n",
    "        if column_schema.dtype == tf.float32:\n",
    "            feature_columns.append(tf.feature_column.numeric_column(feature_name))\n",
    "        elif column_schema.dtype == tf.int64:\n",
    "            feature_columns.append(\n",
    "                tf.feature_column.categorical_column_with_identity(\n",
    "                feature_name, num_buckets=5+1)\n",
    "                                  )\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_eval_col = create_feature_eval_columns(transformed_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[IdentityCategoricalColumn(key='dropofflat', number_buckets=6, default_value=None),\n",
       " IdentityCategoricalColumn(key='dropofflon', number_buckets=6, default_value=None),\n",
       " NumericColumn(key='fare_amount', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='key', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='passengers', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " IdentityCategoricalColumn(key='pickuplat', number_buckets=6, default_value=None),\n",
       " IdentityCategoricalColumn(key='pickuplon', number_buckets=6, default_value=None)]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_eval_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create eval_input_fn\n",
    "\n",
    "def eval_input_receiver_fn():\n",
    "    serialized_tf_example = tf.compat.v1.placeholder(\n",
    "      dtype=tf.string, shape=[None], name='input_example_placeholder')\n",
    "\n",
    "    # This *must* be a dictionary containing a single key 'examples', which\n",
    "    # points to the input placeholder.\n",
    "    receiver_tensors = {'examples': serialized_tf_example}\n",
    "    feat_eval_col = create_feature_eval_columns(transformed_metadata)\n",
    "    feature_spec =  tf.feature_column.make_parse_example_spec(\n",
    "      feat_eval_col)\n",
    "    features = tf.io.parse_example(serialized_tf_example, feature_spec)\n",
    "\n",
    "    return tfma.export.EvalInputReceiver(\n",
    "    features=features,\n",
    "    receiver_tensors=receiver_tensors,\n",
    "    labels=features['fare_amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0807 18:47:43.959941 4734801344 estimator.py:1145] Calling model_fn.\n",
      "I0807 18:47:45.255685 4734801344 estimator.py:1147] Done calling model_fn.\n",
      "I0807 18:47:45.258794 4734801344 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
      "I0807 18:47:45.261008 4734801344 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
      "I0807 18:47:45.262856 4734801344 export_utils.py:170] Signatures INCLUDED in export for Predict: None\n",
      "I0807 18:47:45.265178 4734801344 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
      "I0807 18:47:45.266222 4734801344 export_utils.py:170] Signatures INCLUDED in export for Eval: ['eval']\n",
      "W0807 18:47:45.268890 4734801344 export_utils.py:182] Export includes no default signature!\n",
      "I0807 18:47:45.364635 4734801344 saver.py:1280] Restoring parameters from tft_output_dir/model.ckpt-304\n",
      "I0807 18:47:45.443629 4734801344 builder_impl.py:661] Assets added to graph.\n",
      "I0807 18:47:45.444389 4734801344 builder_impl.py:456] No assets to write.\n",
      "I0807 18:47:45.630475 4734801344 builder_impl.py:421] SavedModel written to: tft_output_dir/eval_export/temp-b'1565171263'/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'tft_output_dir/eval_export/1565171263'"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export the EvalSavedModel\n",
    "eval_export_dir = os.path.join(model_dir, 'eval_export')\n",
    "\n",
    "if tf.io.gfile.exists(eval_export_dir):\n",
    "    tf.io.gfile.rmtree(eval_export_dir)\n",
    "\n",
    "tfma.export.export_eval_savedmodel(\n",
    "    estimator=estimator, export_dir_base=eval_export_dir, \n",
    "    eval_input_receiver_fn=eval_input_receiver_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを分析する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0807 18:51:42.124160 4734801344 saver.py:1280] Restoring parameters from tft_output_dir/eval_export/1565171263/variables/variables\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101857cd364044d893f33164f31e6b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SlicingMetricsViewer(config={'weightedExamplesColumn': 'post_export_metrics/example_count'}, data=[{'slice': '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_shared_model = tfma.default_eval_shared_model(eval_saved_model_path='tft_output_dir/eval_export/1565171263'\n",
    ")\n",
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_model=eval_shared_model,\n",
    "    data_location='./tfrecord/test_transformed-00000-of-00001',\n",
    "    file_format='tfrecords'\n",
    ")\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "tfma.view.render_slicing_metrics(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "何も出てこない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
